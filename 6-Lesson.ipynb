{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение с учителем\n",
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 6, 4\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_data = boston['data']\n",
    "b_target = boston['target']\n",
    "feature_name = boston['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(b_data, columns = feature_name)\n",
    "y = pd.DataFrame(b_target, columns=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.669370269149561"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_test = pd.DataFrame({\n",
    "    \"y_test\": y_test[\"price\"],\n",
    "    \"y_pred\": y_pred.flatten(),\n",
    "})\n",
    "r2_score(check_test[\"y_pred\"], check_test[\"y_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=1000, max_depth=12, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=12, n_estimators=1000, random_state=42)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train.values[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_rfr = model.predict(X_test)\n",
    "y_pred_rfr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>23.6</td>\n",
       "      <td>22.806412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>32.4</td>\n",
       "      <td>31.131464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>13.6</td>\n",
       "      <td>16.339125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>22.8</td>\n",
       "      <td>23.810726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>16.1</td>\n",
       "      <td>17.139521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>20.0</td>\n",
       "      <td>21.832284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>17.8</td>\n",
       "      <td>19.895747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>14.0</td>\n",
       "      <td>14.754118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>19.6</td>\n",
       "      <td>21.240835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>16.8</td>\n",
       "      <td>20.898658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_test     y_pred\n",
       "173    23.6  22.806412\n",
       "274    32.4  31.131464\n",
       "491    13.6  16.339125\n",
       "72     22.8  23.810726\n",
       "452    16.1  17.139521\n",
       "76     20.0  21.832284\n",
       "316    17.8  19.895747\n",
       "140    14.0  14.754118\n",
       "471    19.6  21.240835\n",
       "500    16.8  20.898658"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_test2 = pd.DataFrame({\n",
    "    \"y_test\": y_test[\"price\"],\n",
    "    \"y_pred\": y_pred_rfr,\n",
    "})\n",
    "\n",
    "check_test2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8479049999699443"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(check_test2[\"y_pred\"], check_test2[\"y_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Коэффициент R2 линейной регрессии 0,67 < 0,85 второй модели. Следовательно лучше модель с ансамблем деревьев. \n",
    "# Чем ближе коэффициент к 1 тем сильнее корреляция."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestRegressor in module sklearn.ensemble._forest:\n",
      "\n",
      "class RandomForestRegressor(ForestRegressor)\n",
      " |  RandomForestRegressor(n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest regressor.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of classifying\n",
      " |  decision trees on various sub-samples of the dataset and uses averaging\n",
      " |  to improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"mse\", \"mae\"}, default=\"mse\"\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"mse\" for the mean squared error, which is equal to variance\n",
      " |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      " |      absolute error.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=None\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      whether to use out-of-bag samples to estimate\n",
      " |      the R^2 on unseen data.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int or RandomState, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0, 1)`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeRegressor\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeRegressor\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_prediction_ : ndarray of shape (n_samples,)\n",
      " |      Prediction computed with out-of-bag estimate on the training set.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor, ExtraTreesRegressor\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  The default value ``max_features=\"auto\"`` uses ``n_features``\n",
      " |  rather than ``n_features / 3``. The latter was originally suggested in\n",
      " |  [1], whereas the former was more recently justified empirically in [2].\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      " |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      " |  ...                        random_state=0, shuffle=False)\n",
      " |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  RandomForestRegressor(...)\n",
      " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      " |  [-8.32987858]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestRegressor\n",
      " |      ForestRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestRegressor:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |      \n",
      " |      The predicted regression target of an input sample is computed as the\n",
      " |      mean predicted regression targets of the trees in the forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix or a list of generic objects instead,\n",
      " |          shape = (n_samples, n_samples_fitted),\n",
      " |          where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The R2 score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.barh(feature_name, model.feature_importances_)\n",
    "\n",
    "plt.xlabel(\"Вес признака\")\n",
    "plt.ylabel(\"Признак\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наиболее весомые характеристики - LSTAT и RM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
